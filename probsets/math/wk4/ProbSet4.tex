\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{\footnotesize\textsl{OSM Lab, Summer 2017, Math,Inner Product Space \#4}}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\begin{document}

\begin{flushleft}
   \textbf{\large{Math, Intro to optimization\#4}} \\[5pt]
   OSM Lab instructor, Jorge Barro\\[5pt]
   OSM Lab student, CHEN Anhua\\[5pt]
   Due Wednesday, July 14 at 8:00am
\end{flushleft}

\vspace{5mm}


\begin{enumerate}
	\item(6.1)\\
	The standard form of the problem is as following:\\
	minimize  $-e^{-w^{T}x}$\\
	subject to \begin{equation} \label{eq1}
			\begin{split}
			 w^{T}Aw - w^{T}(x + Ay) \leq -a\\
			 y^{T}w - w^{T}x = b\\
			\end{split}
		      \end{equation}\\

	\item(6.5)\\
	Let $x_{1}, x{2}$ be the amounts of milk bottle and knob to produce respectively and 
	$x = \begin{bmatrix}
	   x_{1}\\
	   x_{2}
	\end{bmatrix} $. Let $w =\begin{bmatrix} 0.7 \\ 0.5 \\ \end{bmatrix} $ and $A =  \begin{bmatrix} 4 & 3 \\ 2 & 1 \\ \end{bmatrix}$ and $b =  \begin{bmatrix} 240,000 \\ 6,000 \\ \end{bmatrix}$The optimization problem is as following:
	\begin{equation} \label{eq2}
			\begin{split}
			\text{minimize}   & \:  -w^{T}x\\
			\text{subject to}& \:  Ax \leq b\\
			\end{split}
	 \end{equation}\\
			 
	 \item(6.6)\\
	$Df(x, y) = \begin{bmatrix} 6xy + 4y^2 + y \\ 3x^2 + 8xy +x \end{bmatrix} = \textbf{0} \implies \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix},  \begin{bmatrix} 0 \\ -\frac{1}{4} \end{bmatrix},  \begin{bmatrix} -\frac{1}{3} \\ 0 \end{bmatrix},  \begin{bmatrix} -\frac{1}{9} \\ -\frac{1}{12} \end{bmatrix}  $\\
	We also have $D^{2}f(x, y) = \begin{bmatrix} 6y & 6x + 8y + 1\\ 6x + 8y + 1 & 8x \\\end{bmatrix}$. Since $x, y \in \mathbb{R}$, we examine the $D^{2}f(x, y)$ by plugging in the critical points and according to \textbf{Remark 6.2.10}, we could find that $\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ -\frac{1}{4} \end{bmatrix} and  \begin{bmatrix} -\frac{1}{3} \\ 0 \end{bmatrix}$ are saddle points while $\begin{bmatrix} -\frac{1}{9} \\ -\frac{1}{12}\end{bmatrix}$ is the local maximum.\\

	\item(6.11)\\
	By using the Newton method, $q(x) = f(x_{0}) + f^{'}(x_{0})(x - x_{0}) + \frac{1}{2}f^{''}(x_{0})(x - x_{0})^{2} = ax_{0}^2 + bx_{0} + c +(2ax_{0} + b)(x - x_{0}) + a(x - x_{0})^{2} = zx^2 + bx +c$. Therefore, the minimizer of $q(x)$ coincides with that of the orignial funciton and the optimization problem will be done in one iteration.\\

$s_{L} = \frac{wL}{PY}$\\

$s_{K_{j}}  = \frac{ R_{j}P_{j}K_{j}}{PY}$

$\alpha_{L} = s_{L} $ \\

$ \alpha_{K_{j}}  = (1 -  s_{L}) \frac{R_{j}P_{j}K_{j}}{\sum R_{j}P_{j}K_{j}}$\\
	
$\alpha_{L} =  \frac{s_{L}}{s_{L} + \sum s_{K_{j}}} $ \\

$\alpha_{K_{j}}=  \frac{s_{K_{j}}}{s_{L} + \sum s_{K_{j}}} $ \\
	
$\frac{s_{\pi}}{s_{L} + \sum s_{K_{j}}} =  \mu $\\



\end{enumerate}

\vspace{25mm}



\end{document}
