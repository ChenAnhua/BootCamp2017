\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{\footnotesize\textsl{OSM Lab, Summer 2017, Math PS \#1}}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\begin{document}

\begin{flushleft}
   \textbf{\large{Math, Problem Set \#1, Probability Theory}} \\[5pt]
   OSM Lab instructor, Karl Schmedders \\[5pt]
   OSM Lab student, CHEN Anhua (Peter)\\[5pt]
   Due Monday, June 26 at 8:00am
\end{flushleft}

\vspace{5mm}

\begin{enumerate}
	\item {\bf Exercises from chapter.} Do the following exercises in Chapter 3 of \citet{HJ17}: 3.6, 3.8, 3.11, 3.12 (watch this movie \href{https://www.youtube.com/watch?v=Zr_xWfThjJ0}{clip}), 3.16, 3.33, 3.36.
			\item[(3.6)]
			$P(A)$\\
			$= P(A \cap \Omega)$\\
			$= P(\cup_{i \in I} B_i \cap A)$\\
			$= \sum_{i \in I} P(A \cap B_i)$  (Finite additivity)

			\item[(3.8)]
			$P(\cup_{k = 1}^{n} E_k)$\\
			$= 1 - P((\cup_{k = 1}^{n} E_k)^c)$\\
			$= 1 - P(E_1^c \cap ... \cap E_n^c)$  (De Morgan's law)\\
			$= 1 - \prod_{k = 1}^{n} (1 - P(E_k))$\\

			\item[(3.11)]
			$P(s = crime | s tested+)$\\
			$= \frac{P(s = crime, s tested+)}{P(s tested +)}$\\
			$= \frac{P(s tested+ | s = crime)P(s = crime)}{P(s tested +)}$\\
			$= \frac{\frac{1}{250} \times \frac{1}{250000000} }{\frac{1}{3000000}}$\\
			$= 0.000048$\\


			\item[(3.12)]
			For the three-door case, assume contestant chooses door 1 and Monty opened door 2 behind which there is a goat. Then $P(prize 1 | open 2) = \frac{P(open 2 | prize 1)P(prize 1)}{P(open 2)}$. Since $P(open 2) = P(open 2 | prize 1)P(prize 1) + P(open 2 | prize not in 1)P(prize not in 1) = \frac{1}{2}\times \frac{1}{3} + \frac{1}{2}\times \frac{2}{3} = \frac{1}{2}$, the probabity of winning if he chooses to stay would be: $\frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}$. Therefore the probability to win if he chooses to alter would be $\frac{2}{3}$.\\
			By using the same logic, if there is 10 doors. $P(prize 1 | open 2 to 9) = \frac{P(open 2 to 9 | prize 1)P(prize 1)}{P(open 2 to 9)} = \frac{\frac{8}{9} \times \frac{1}{10}}{\frac{8}{9}} = \frac{1}{10}$. The probability to win if alterign would be $\frac{9}{10}$


			\item[(3.16)]
			$var(X)$\\
			$= E((X - \mu)^2)$\\
			$= E(X^2) - 2E(\mu X) + E(\mu^2)$\\
			$= E(X^2) + \mu^2 - 2\mu^2$\\
			$= E(X^2) - \mu^2$


			\item[(3.33)]
			Since $E(B) = np$ and $var(B) = np(1 - p)$\\
			$P(|\frac{B}{n} - p| \geq \varepsilon)$\\
			$= P(|N - np| \geq n\varepsilon)$\\
			$\leq \frac{np(1-p)}{n\varepsilon^2}$ (Chebyshev inequality)\\
			$= \frac{p(1-p}{\varepsilon^2}$\\

			\item[(3.36)]
			We define  
			 \begin{equation}
			       X = 
			        \begin{cases}
			            1 & \text{enrolled} \\
			            0 & \text{not enrolled}
			        \end{cases}
 			   \end{equation}\\
			$p = \mu = 0.801$\\
			According to the Central Limit Theorem:
			$P(X > 5500) = 1 - \Phi (\frac{5500 - 6242\times 0.801}{\sqrt{0.801\times (1 - 0.801)} \times \sqrt{6242}})$
	\item Construct examples of events $A$, $B$, and $C$, each of probability strictly between 0 and 1, such that
   		\begin{itemize}
			\item[(a)] $P(A  \cap B) = P(A)P(B)$, $P(A  \cap C) = P(A)P(C)$, $P(B  \cap C) = P(B)P(C)$, but $P(A  \cap B \cap C) \neq P(A)P(B)P(C)$.\\
			Let $\Omega = \{ 1, 2, 3, 4, 5, 6, 7, 8\}$, each event has a probability of $\frac{1}{8}$ to happen\\
			$A = \{1, 2, 3, 4\}$,
			$B = \{1, 2, 5, 6\}$,
			$C = \{1, 2, 7, 8\}$\\
			$P(A \cap B) = \frac{1}{4} = P(A)P(B)$\\
			$P(A \cap C) = \frac{1}{4} = P(A)P(C)$\\
			$P(B \cap C) = \frac{1}{4} = P(B)P(C)$\\
			However $P(A\cap B \cap C) = \frac{1}{4}  \neq  P(A) P(B)P(C) = \frac{1}{8}$\\
						
			\item[(b)] $P(A  \cap B) = P(A)P(B)$, $P(A  \cap C) = P(A)P(C)$, $P(A  \cap B \cap C) = P(A)P(B)P(C)$, but $P(B  \cap C) \neq P(B)P(C)$. (Hint: You can let $\Omega$ be a set of eight equally likely points.)\\
			
			Let $\Omega = \{ 1, 2, 3, 4, 5, 6, 7, 8\}$, each event has a probability of $\frac{1}{8}$ to happen\\
			$A = \{1, 2, 3, 4\}$,
			$B = \{1, 2, 5, 6\}$,
			$C = \{1, 3, 7, 8\}$\\
			$P(A \cap B) = \frac{1}{4} = P(A)P(B)$\\
			$P(A \cap C) = \frac{1}{4} = P(A)P(C)$\\
			$P(A \cap B \cap C) = \frac{1}{8} = P(A)P(B)P(C)$\\
			However $P(B \cap C) = \frac{1}{8} \neq P(B)P(C)$\\
			
		\end{itemize}
   	\item Prove that Benford's Law is, in fact, a well-defined discrete probability distribution.\\
		\item[1)] 
		The lower bound of $log(1 + \frac{1}{d}) = log1 = 0$ and bounded above by $log2 < 1$\\

		\item[2)]
		$\sum_{d = 1}^{9} log(1 + \frac{1}{d}) = 1$\\

		\item[3)]
		For finite additivity additivity, we get $P(\cup_{d = 1}^{\infty} E_d) = \sum_{d = 1}^{\infty} P(E_d)$\\

   	\item A person tosses a fair coin until a tail appears for the first time. If the tail appears on the $n$th flip, the person wins $2^n$ dollars. Let the random variable $X$ denote the player's winnings.
		\begin{itemize}
			\item[(a)] (St. Petersburg paradox) Show that $E[X]= + \infty$.\\
			
			$E(x) = \sum_{n = 1}^{\infty} 0.5^n2^n$\\
			$= +\infty$

			\item[(b)] Suppose the agent has log utility. Calculate $E[\ln X]$.\\
			
			$E[ln X] = \sum_{n = 1}^{\infty} 0.5^nln(2^n)$\\
			$= ln2\sum_{n = 1}^{\infty} \frac{n}{2^n}$\\
			$= 2ln2$
		\end{itemize}
	
	\item (Siegel's paradox) Suppose the exchange rate between USD and CHF is 1:1. Both a U.S. investor and a Swiss investor believe that a year from now the exchange rate will be either $1.25:1$ or $1:1.25$, with each scenario having a probability of 0.5. Both investors want to maximize their wealth in their respective home currency (a year from now) by investing in a risk-free asset; the risk-free interest rates in the U.S. and in Switzerland are the same. Where should the two investors invest?\\
		
	If the investor investor invest in their own currency, only get the risk-free interest rate: $1 + r_f$. If they invest in foreign currency, they will get: $0.5 \times (1 + r_f) \times 1.25 + 0.5 \times (1 + r_f) \times \frac{1}{1.25} = 1.025 \times (1 + r_f)$. Therefore, they should all invest in the foreign currency.

\item Consider a probability measure space with $\Omega = [0,1]$.
		\begin{itemize}
			\item[(a)] Construct a random variable $X$ such that $E[X] < \infty$ but $E[X^2] = \infty$.\\
			Since $var(X) = E(X^2) - [E(x)]^2$, this means that random variable $x$ will have an infinity variance. After searching on google, I find Pareto distribution statisfies the requirements that with a finite mean and infinite variance. \\

			\item[(b)] Construct random variables $X$ and $Y$ such that $P(X>Y)>\frac{1}{2}$ but $E[X]<E[Y]$.\\
			
			
			\item[(c)] Construct random variables $X$, $Y$, and $Z$ such that\\ $P(X>Y) P(Y>Z) P(X>Z) > 0$ and 						$E(X)=E(Y)=E(Z)=0$.\\
			
			Let $X, Y, Z$ be normally distributed random variables with $0$ means and same variance.
		\end{itemize}

	\item Let the random variables $X$ and $Z$ be independent with $X \sim N(0,1)$ and $P(Z=1)=P(Z=-1)=\frac{1}{2}$. 			Define $Y= XZ$ as the product of $X$ and $Z$. Prove or disprove each of the following statements.
		\begin{itemize}
			\item[(a)] $Y \sim N(0,1)$.\\
			It's true. In order to prove $X$ and $Y$ have same distribution, we just need to make sure their PDF or CDF are the same\\
			$P(Y < x)$\\
			$= P(Z = 1)P(X < x) + P(Z = -1)P(X > -x)$\\
			$= P(X<x)$ since normal distribution is symmetrical around mean\\

			\item[(b)] $P(|X|=|Y|)=1$.\\
			It's true. Proof is omitted.\\

			\item[(c)] $X$ and $Y$ are not independent.\\
			it's true because $P(X < a| Y < a)  = \frac{1}{2}$, when $a \leq = 0$ while $P(x < a) < \frac{1}{2}$\\
			Therefore, $P(X < a| Y < a) \neq P(x < a)$, indicating $X$ and $Y$ are not independent.\\
			\item[(d)] $Cov[X,Y]=0$.\\
			it's true.\\
			$cov[X, Y]$
			$= cov[X, XZ]$\\
			$= E[XXZ] - E[X]E[XZ]$\\
			$= E[XXZ] - E[Z]E[X]E[X]$ since $X$ and $E$ are independent and $E[Z] = 0$\\
			$= E[XY]$\\
			$= cov[X, Y] - E[X]E[Y]$\\
			$= 0$\\

			\item[(e)] If $X$ and $Y$ are normally distributed random variables with $Cov[X,Y]=0$, then $X$ and $Y$ 					must be dependent.\\
			it's false. (a) to (d) provide an example where $X$ and $Y$ are not independent.\\

		\end{itemize}

	\item Let the random variables $X_i$, $i=1,2,\ldots,n,$ be i.i.d.\ having the uniform distribution on $[0,1]$, denoted $X_i \sim U[0,1]$. Consider the random variables $m=\min\{X_1,X_2,\ldots,X_n\}$ and $M=\max\{X_1,X_2,\ldots,X_n\}$. For both random variables $m$ and $M$, derive their respective cumulative distribution (cdf), probability density function (pdf), and expected value.\\
			\item[(a)] m\\
			CDF:\\
			$P(m \leq x)$\\
			$= 1 - P(m \geq x)$\\
			$= 1 - P(\text{All Xs are larger than x})$\\
			$= 1 - (1 - x)^n$\\
			PDF:\\
			$n(1 - x)^{n - 1}$\\
			Expected value: \\
			$$E(m) = \int_{0}^{1} n(1 - x)^{n - 1} dx = \frac{1}{n + 1}$$
			\item[(b)] M\\
			CDF:\\
			$P(M \leq x)$\\
			$=  P(\text{All Xs are smaller than x})$\\
			$= (x)^n$\\
			PDF:\\
			$n(x)^{n - 1}$\\
			Expected value: \\
			$$E(M) = \int_{0}^{1} n(x)^{n - 1} dx = \frac{n}{n + 1}$$


	\item You want to simulate a dynamic economy (e.g., an OLG model) with two possible states in each period, a ``good'' state and a ``bad'' state. In each period, the probability of both shocks is $\frac{1}{2}$. Across periods the shocks are independent. Answer the following questions using the Central Limit Theorem and the Chebyshev Inequality.
		\begin{itemize}
			\item[(a)] What is the probability that the number of good states over 1000 periods differs from 500 by at most 2\%?\\
 		           We define  
			 \begin{equation}
			       X = 
			        \begin{cases}
			            1 & \text{state is good} \\
			            0 & \text{state is bad}
			        \end{cases}
 			   \end{equation}\\
			By Central Limit Theorem:\\
			$P( | \sum_{n=1}^{1000} X_n - 500| \leq 10)$ \\
			$= \Phi (\frac{10}{0.5\sqrt{1000}}) - \Phi (\frac{-10}{0.5\sqrt{1000}})$\\
			$= 0.47$
			\item[(b)] Over how many periods do you need to simulate the economy to have a probability of at least 0.99 that the proportion of good states differs from $\frac{1}{2}$ by less than 1\%?\\
			According to the Chebyshev inequality:\\
			$P( | X - 0.5 | \geq \varepsilon) \leq \frac{\sigma^2}{n\varepsilon^2}$ \\
			$\varepsilon = 0.005$ and $\sigma^2 = 0.25$\\
			Therefore, in order to fufill the condition that $P( | X - 0.5 | \geq \varepsilon) \leq 0.01$, we could let $\frac{\sigma^2}{n\varepsilon^2} = 0.01$\\
			We will get $n = 1000000$.
		\end{itemize}

	\item If $E[X]<0$ and $\theta \neq 0$ is such that $E[e^{\theta X}]=1$, prove that $\theta > 0$.\\
	Proof: \\
	we will use Jensen inequality to prove. Let $f(x) = e^{\theta x}$. Since the second order derivative of $f(x)$ is always positive, $f(x)$ is a convex function. According to Jensen's inequality, $E[e^{\theta x}] \geq e^{\theta E[x]}$. Since $E[e^{\theta X}]=1$, $ e^{\theta E[x]} \leq 1$. This means $\theta E[x] \leq 0$ and $\theta \neq 0$. Since $E[x] < 0$, $\theta > 0$.
\end{enumerate}

\vspace{25mm}

\bibliography{ProbStat_probset}

\end{document}
